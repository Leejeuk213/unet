{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인공지능 과제2  \n",
    "201901761 이제욱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cnn이란? \n",
    "* 합성곱 신경망\n",
    "* 인간의 시신경 구조를 모방한 네트워크로 이미지를 인식하고 패턴을 찾는데 유용하다.\n",
    "* 구현할 때 conv2d가 아닌 conv1d 등으로도 사용해 이미지뿐만 아닌 시계열 데이터에 대해서도 모델을 만들 수 있다.\n",
    "\n",
    "cnn은 완전 연결 계층으로 이미지를 학습할 때의 문제점으로부터 출발됐다. 완전연결 계층은 입력 데이터의 형태로 1차원 형태의 데이터를 받는데  \n",
    "이미지는 2차원 컬러 사진일 경우 3차원이기에 이것을 flatten해서 1차원 데이터로 만들어서 입력으로 넣어주게 된다.  \n",
    "\n",
    "이 과정에서 이미지의 공간적/지역적 정보가 소실되기에 신경망이 특징을 추출하고 학습하는데에 있어 비효율적이고 정확도를 높이는데에 한계가 있다.  \n",
    "\n",
    "이러한 문제점으로 고안된 해결책이 cnn이다. 이미지 데이터를 그대로 받아와 공간적/지역적 정보를 유지한 채 이미지의 패턴을 추출한다.  \n",
    "\n",
    "1. 이미지 패턴 추출 영역  \n",
    "이미지 패턴 추출 영역은 convolution -> 활성화 -> pooling의 순으로 이루어진다. 모델을 어떻게 만드냐에 따라 이러한 구간이 여러번 반복 될 수 있다.  \n",
    "\n",
    "1-1 convolution \n",
    "필터로 특정 패턴이 있는지 훑으면서 마킹한다. 결과값으로 숫자가 나오는데 이 숫자를 활성화 함수에 넣어 나온 값으로 이미지를 새로 그린다.  \n",
    "활성화 함수론 주로 relu를 사용한다.  \n",
    "\n",
    "하나의 convolution 계층에는 이미지의 채널 개수만큼 필터가 존재한다. rgb 이미지면 3개인 것이다.  이 필터를 적용해 합성곱 연산을 해  \n",
    "출력 이미지가 생성이 된다.  \n",
    "\n",
    "이런 합성곱 연산을 할 떄 주게 되는 parameter로는 3개가 있는데 필터의 사이즈, stride, padding이 있다.  \n",
    "\n",
    "* stride  \n",
    "stride는 합성곱 연산을 할 때 filter가 어느 정도의 간격으로 이동하는지를 나타낸다. stride를 어떻게 설정하느냐에 따라 output image의 size가 달라진다.  \n",
    "\n",
    "* padding  \n",
    "합성곱 연산을 할 때 output image의 size가 점점 줄어지게 되고, input image의 가장자리에 위치한 pixel들의 정보가 점점 사라지게 된다.  \n",
    "이런 문제점을 해결하기 위해 이용되는 것이 padding이고 cnn에선 주로 zero-padding이 이용된다.  \n",
    "이미지의 가장자리에 0의 값을 갖는 pixel을 추가해 pixel의 손실이 없게끔 하는 것이다.  \n",
    "\n",
    "1-2 활성화 함수  \n",
    "합성곱을 통해 나온 이미지 pixel 값들에 element-wise하게 전부 적용한다.  \n",
    "\n",
    "1-3 pooling  \n",
    "이미지의 크기를 계속 유지한 채 완전 연결 계층으로 가게 된다면 연산량이 매우 많아지게 된다. 크기를 줄이면서 특정 패턴을 강조해야 하는데  \n",
    "그 역할을 pooling layer에서 하게 된다.  \n",
    "\n",
    "pooling 방식은 크게 세가지 max, average, min이 있는데 cnn에서는 주로 max pooling을 사용한다.  \n",
    "뉴런이 가장 큰 신호에 반응하는 것과 max pooling의 작동이 유사하기 때문이다.  \n",
    "\n",
    "일반적으로 pooling layer의 stride는 pooling 연산을 하기 원하는 영역의 크기만큼 움직인다.  \n",
    "주로 cnn에서 pooling은 2,2 size만큼 하게 되기에 output image의 size는 절반이 된다.  \n",
    "\n",
    "2. 완전 연결 계층  \n",
    "평범한 neural network이다. 입력 데이터가 1차원으로 우선 flatten되고 원하는 활성화 함수에 의해 결과 값이 처리된다.    \n",
    "\n",
    "앞선 작업들이 이미지의 패턴을 추출하기 위한 작업이였다면 해당 계층에선 분류 작업을 하게 된다.  \n",
    "추출된 패턴이 무엇을 의미하는지 분류하는 것이다.  \n",
    "\n",
    "2-1.  flatten  \n",
    "이미지 데이터를 1차원 데이터로 변경  \n",
    "\n",
    "2-2. 활성화 함수  \n",
    "\n",
    "softmax 혹은 relu가 사용된다.  \n",
    "\n",
    "\n",
    "\n",
    "![nn](cnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unet이란?  \n",
    "\n",
    "앞서 말한 기본 cnn 구조는 분류작업에 사용되지만 unet은 cnn의 응용으로 주로 이미지 세그멘테이션에 이용된다.    \n",
    "\n",
    "unet은 'U-Net: Convolutional Networks for Biomedical Image Segmentation' 이라는 논문에서 제안한 구조로서 매우 적은 수의 학습 데이터로도 정확한 이미지 세그멘테이션 성능을 보여주었으며 ISBI 세포 추적 챌린지 2015에서 큰 점수 차이로 우승했다.  \n",
    "\n",
    "unet은 auto encoder와 같은 encoder-decoder 기반 모델에 속한다.   \n",
    "보통 인코딩 단계에서는 입력 이미지의 특징을 포착할 수 있도록 채널의 수를 늘리면서 차원을 축소해 나가며,   \n",
    "디코딩 단계에서는 저차원으로 인코딩된 정보만 이용하여 채널의 수를 줄이고 차원을 늘려서 고차원의 이미지를 복원한다.   \n",
    "하지만 인코딩 단계에서 차원 축소를 거치면서 이미지에 대한 위치 정보를 잃게 되고,   \n",
    "디코딩 단계에서도 저차원의 정보만을 이용하기 때문에 위치 정보 손실을 회복하지 못하게 된다.  \n",
    "\n",
    "unet의 차이점은 저차원 뿐만 아니라 고차원 정보도 이용하여 이미지의 특징을 추출함과 동시에 정확한 위치 파악도 하는 것이다.   \n",
    "이를 위해서 인코딩 단계의 각 레이어에서 얻은 특징을 디코딩 단계의 각 레이어에 합치는(concatenation) 방법을 사용한다.   \n",
    "인코더 레이어와 디코더 레이어의 직접 연결을 스킵 연결(skip connection)이라 한다.  \n",
    "\n",
    "이런 encoder와 decoder 사이의 스킵 연결을 표현한 구조가 'u'자 여서 unet으로 이름이 붙여지게 됐다.  \n",
    "\n",
    "unet은 세 영역으로 나뉜다 encoder, bridge, decoder  \n",
    "\n",
    "1. encoder  \n",
    "\n",
    "1-1 conv 3x3, relu  \n",
    "각 encoding step마다 3x3 convolution을 두 차례 반복하고 각각의 convolution에는 relu가 포함된다.  \n",
    "padding을 하지 않고 stride는 2이다. padding을 하지 않기에 output map이 조금씩 줄어든다.  \n",
    "\n",
    "1-2 max pool 2x2  \n",
    "2차레의 convolution과 relu가 끝난 다음 2x2 사이즈 stride 2의 max pool을 한다.  \n",
    "계산 결과 절반으로 줄어든 output map을 갖게 된다.  \n",
    "\n",
    "encoder는 1024의 채널을 가질 때까지 반복된다.  \n",
    "\n",
    "2. bridge  \n",
    "\n",
    "bridge에선 pooling 없이 두 번 convolution 연산만 하게 된다.  \n",
    "\n",
    "3. decoder  \n",
    "\n",
    "각 step마다 2x2 deconvolution을 수행한다. 이 때 output map의 size가 두 배로 늘어나고 채널 수는 절반으로 줄어든다.  \n",
    "\n",
    "그 후에 스킵 연결을 통해 encoder 층의 map을 복사하고 deconvolution을 통해 올라온 map과 concatenation을 해준다.  \n",
    "\n",
    "이를 통해 저차원 이미지정보 뿐만 아니라 고차원 이미지도 활용해 위치 정보를 보다 정확히 파악할 수 있다.  \n",
    "\n",
    "3-1 2x2 deconvolution\n",
    "2x2 deconvolution 연산을 통해 채널 수를 절반으로 줄이고 map의 사이즈를 두 배로 한다.  \n",
    "\n",
    "3-2 concatenate  \n",
    "스킵 연결을 통해 올라온 encoder 층의 map과 deconvolution output 맵을 합친다.  \n",
    "\n",
    "3-3 conv 3x3, relu  \n",
    "각 encoding step마다 3x3 convolution을 두 차례 반복하고 각각의 convolution에는 relu가 포함된다.  \n",
    "padding을 하지 않고 stride는 2이다. padding을 하지 않기에 output map이 조금씩 줄어든다.  \n",
    "\n",
    "이후에 마지막에는 1x1 convolution을 해 세그멘테이션 맵을 생성한다.\n",
    "\n",
    "\n",
    "\n",
    "![nn](unet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다음과 같은 이미지들을 예측해서 출력  \n",
    "\n",
    "test image가 들어오면 세포벽을 기준으로 분류된 image segmentation된 결과를 하나 출력하고  \n",
    "세포벽을 분류한 결과도 출력  \n",
    "\n",
    "![nn](pred_exam1.png)\n",
    "\n",
    "\n",
    "\n",
    "![nn](pred_exam2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필요한 라이브러리들 import \n",
    "\n",
    "없을 경우 requirments.txt 파일에서 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\python\\lib\\site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tifffile in c:\\python\\lib\\site-packages (2023.9.26)\n",
      "Requirement already satisfied: numpy in c:\\python\\lib\\site-packages (from tifffile) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorbord (from versions: none)\n",
      "ERROR: No matching distribution found for tensorbord\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\python\\lib\\site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\python\\lib\\site-packages (from scipy) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticdeform in c:\\python\\lib\\site-packages (0.5.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy in c:\\python\\lib\\site-packages (from elasticdeform) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\python\\lib\\site-packages (from elasticdeform) (1.11.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\python\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\python\\lib\\site-packages (from opencv-python) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\python\\lib\\site-packages (2.13.0)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\python\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.24.3)\n",
      "Requirement already satisfied: setuptools in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\python\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\python\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.23.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\python\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\python\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\python\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy\n",
    "%pip install tifffile\n",
    "%pip install tensorbord\n",
    "%pip install scipy\n",
    "%pip install elasticdeform\n",
    "%pip install opencv-python\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime # 폴더명 저장할 때 날짜 돌린 시간 분을 이름으로 저장하기 위해 \n",
    "import tifffile as tiff # tif 이미지 데이터셋을 불러오기 위함\n",
    "from scipy.ndimage import rotate # data augmentation중 회전할 때 사용하기 위함 \n",
    "import cv2 # 이미지파일 처리할 때 필요한 라이브러리 \n",
    "import elasticdeform # data augmentation에서 탄성 변형할 때 사용하기 위함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 처리 함수들과 그 외 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folders(cur_time=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    현재 시간을 기반으로 'logs' 및 'test' 디렉토리를 생성.\n",
    "\n",
    "    매개변수:\n",
    "    - cur_time: 현재 시간 문자열.\n",
    "\n",
    "    반환값:\n",
    "    - log_dir: 로그를 저장할 디렉토리 경로.\n",
    "    - test_dir: 테스트 결과를 저장할 디렉토리 경로.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 로그 및 테스트 디렉토리 경로 생성\n",
    "    log_dir = os.path.join('logs', '{}'.format(cur_time))\n",
    "    test_dir = os.path.join('test', '{}'.format(cur_time))\n",
    "\n",
    "    # 로그 디렉토리가 존재하지 않으면 생성\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    # 테스트 디렉토리가 존재하지 않으면 생성\n",
    "    if not os.path.isdir(test_dir):\n",
    "        os.makedirs(test_dir)\n",
    "\n",
    "    # 생성된 디렉토리 경로 반환\n",
    "    return log_dir, test_dir\n",
    "\n",
    "def normalize_uint8(x, x_min=0, x_max=12, fit=255):\n",
    "    \n",
    "    \"\"\"\n",
    "    입력 데이터를 [0, 255] 범위로 정규화하고 uint8로 변환.\n",
    "\n",
    "    매개변수:\n",
    "    - x: 정규화할 입력 데이터.\n",
    "    - x_min: 입력 데이터의 최소값 (기본값은 0).\n",
    "    - x_max: 입력 데이터의 최대값 (기본값은 12).\n",
    "    - fit: 정규화된 출력의 최대값으로, 정규화된 데이터를 원하는 범위로 스케일링하는 데 사용 (기본값은 255).\n",
    "\n",
    "    반환값:\n",
    "    - x_norm: 정규화 및 uint8로 변환된 데이터.\n",
    "    \"\"\"\n",
    "    x_norm = np.uint8(fit * (x - x_min) / (x_max - x_min))\n",
    "    return x_norm\n",
    "\n",
    "def pseudoColor(label, thickness=3):\n",
    "    \n",
    "    # 레이블 이미지를 가상의 색상으로 변환하여 반환.\n",
    "    # 이미지 세그멘테이션 결과를 시각화할 때 사용.\n",
    "    \n",
    "    # 입력된 레이블 이미지를 복사\n",
    "    img = label.copy()\n",
    "    \n",
    "    # 레이블 이미지에서 외곽선을 찾음\n",
    "    contours, hierachy = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # 이미지를 3채널(RGB)로 변경\n",
    "    img = np.dstack((img, img, img))\n",
    "    \n",
    "    # 찾은 외곽선에 대해 가상의 색상을 입힘\n",
    "    for i in range(len(contours)):\n",
    "        cnt = contours[i]\n",
    "        cv2.drawContours(img, [cnt], contourIdx=-1, color=(0, 255, 0), thickness=thickness)\n",
    "        cv2.fillPoly(img, [cnt], color=randomColors(i))\n",
    "\n",
    "    return img\n",
    "\n",
    "def randomColors(idx):\n",
    "    \n",
    "    # 인덱스를 이용하여 가상의 색상을 선택하고 반환.\n",
    "    \n",
    "    # 각 레이블에 대한 가상의 색상을 미리 정의합니다.\n",
    "    Sky = [128, 128, 128]\n",
    "    Building = [128, 0, 0]\n",
    "    Pole = [192, 192, 128]\n",
    "    Road = [128, 64, 128]\n",
    "    Pavement = [60, 40, 222]\n",
    "    Tree = [128, 128, 0]\n",
    "    SignSymbol = [192, 128, 128]\n",
    "    Fence = [64, 64, 128]\n",
    "    Car = [64, 0, 128]\n",
    "    Pedestrian = [64, 64, 0]\n",
    "    Bicyclist = [0, 128, 192]\n",
    "    DarkRed = [0, 0, 139]\n",
    "    PaleVioletRed = [147, 112, 219]\n",
    "    Orange = [0, 165, 255]\n",
    "    Teal = [128, 128, 0]\n",
    "\n",
    "    # 정의된 색상을 리스트로 묶어놓은 딕셔너리를 만듬\n",
    "    color_dict = [Sky, Building, Pole, Road, Pavement,\n",
    "                  Tree, SignSymbol, Fence, Car, Pedestrian,\n",
    "                  Bicyclist, DarkRed, PaleVioletRed, Orange, Teal]\n",
    "    \n",
    "    # 인덱스를 이용하여 색상을 선택하고 반환\n",
    "    return color_dict[idx % len(color_dict)]\n",
    "\n",
    "\n",
    "def aug_translate(img, label, wmap, max_factor=1.2):\n",
    "\n",
    "    # 이미지를 무작위로 확대한 후 무작위로 자르는 방식으로 데이터를 증강.\n",
    "    # 이미지를 이동시키는 방식\n",
    "    \n",
    "    # 이미지를 무작위로 확대\n",
    "    resize_factor = np.random.uniform(low=1.,  high=max_factor)\n",
    "    img_bigger = cv2.resize(src=img.copy(), dsize=None, fx=resize_factor, fy=resize_factor,\n",
    "                            interpolation=cv2.INTER_LINEAR)\n",
    "    label_bigger = cv2.resize(src=label.copy(), dsize=None, fx=resize_factor, fy=resize_factor,\n",
    "                              interpolation=cv2.INTER_NEAREST)\n",
    "    wmap_bigger = cv2.resize(src=wmap.copy(), dsize=None, fx=resize_factor, fy=resize_factor,\n",
    "                             interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 수평 및 수직 축에 대한 무작위 위치를 생성\n",
    "    h_bigger, w_bigger = img_bigger.shape\n",
    "    h_star = np.random.randint(low=0, high=h_bigger-img.shape[0]+1, size=None)\n",
    "    w_star = np.random.randint(low=0, high=w_bigger-img.shape[1]+1, size=None)\n",
    "\n",
    "    # 확대된 이미지에서 이미지를 자름. 레이블 및 가중치 맵도 동일하게 자름\n",
    "    img_crop = img_bigger[h_star:h_star+img.shape[1], w_star:w_star+img.shape[0]]\n",
    "    label_crop = label_bigger[h_star:h_star+img.shape[1], w_star:w_star+img.shape[0]]\n",
    "    wmap_crop = wmap_bigger[h_star:h_star+img.shape[1], w_star:w_star+img.shape[0]]\n",
    "\n",
    "    return img_crop, label_crop, wmap_crop\n",
    "\n",
    "\n",
    "def aug_flip(img, label, wmap):\n",
    "\n",
    "    # 무작위로 수평 뒤집기 여부를 결정 0.5보다 크면 뒤집기\n",
    "    if np.random.uniform(low=0., high=1.) > 0.5:\n",
    "        img_hflip = cv2.flip(src=img, flipCode=0)\n",
    "        label_hflip =  cv2.flip(src=label, flipCode=0)\n",
    "        wmap_hflip = cv2.flip(src=wmap, flipCode=0)\n",
    "    else:\n",
    "        img_hflip = img.copy()\n",
    "        label_hflip = label.copy()\n",
    "        wmap_hflip = wmap.copy()\n",
    "\n",
    "    # 무작위로 수직 뒤집기 여부를 결정 0.5보다 크면 뒤집기\n",
    "    if np.random.uniform(low=0., high=1.) > 0.5:\n",
    "        img_vflip = cv2.flip(src=img_hflip, flipCode=1)\n",
    "        label_vflip = cv2.flip(src=label_hflip, flipCode=1)\n",
    "        wmap_vflip = cv2.flip(src=wmap_hflip, flipCode=1)\n",
    "    else:\n",
    "        img_vflip = img_hflip.copy()\n",
    "        label_vflip = label_hflip.copy()\n",
    "        wmap_vflip = wmap_hflip.copy()\n",
    "\n",
    "    return img_vflip, label_vflip, wmap_vflip\n",
    "\n",
    "def aug_rotate(img, label, wmap):\n",
    "    \n",
    "    # 이미지를 무작위로 회전하여 데이터를 증강.\n",
    "\n",
    "    # 0에서 360도까지 무작위로 회전 각도를 선택 \n",
    "    angle = np.random.randint(low=0, high=360)\n",
    "    img_rotate = rotate(input=img, angle=angle, axes=(0, 1), reshape=False, order=3, mode='reflect')\n",
    "    label_rotate = rotate(input=label, angle=angle, axes=(0, 1), reshape=False, order=0, mode='reflect')\n",
    "    wmap_rotate = rotate(input=wmap, angle=angle, axes=(0, 1), reshape=False, order=0, mode='reflect')\n",
    "\n",
    "    # 회전 후 레이블을 이진화 127.5를 기준으로 0 또는 255로 변환\n",
    "    ret, label_rotate = cv2.threshold(src=label_rotate, thresh=127.5, maxval=255, type=cv2.THRESH_BINARY)\n",
    "\n",
    "    return img_rotate, label_rotate, wmap_rotate\n",
    "\n",
    "\n",
    "def aug_elastic_deform(img, label, wmap):\n",
    "    \n",
    "    # 이미지를 탄성 변형하여 데이터를 증강.\n",
    "\n",
    "    # deform_random_grid 함수를 사용하여 무작위 탄성 변형을 수행\n",
    "    img_distort, label_distort, wmap_distort = elasticdeform.deform_random_grid(X=[img, label, wmap],\n",
    "                                                                                sigma=10,\n",
    "                                                                                points=3,\n",
    "                                                                                order=[2, 0, 0],\n",
    "                                                                                mode='mirror')\n",
    "\n",
    "    return img_distort, label_distort, wmap_distort\n",
    "\n",
    "def cropping(img, label, wmap, input_size, output_size, is_extend=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    입력 이미지, 레이블 및 가중치 맵에서 무작위 위치에서 부분 이미지를 추출.\n",
    "\n",
    "    매개변수:\n",
    "    - img: 입력 이미지.\n",
    "    - label: 이미지와 연관된 레이블.\n",
    "    - wmap: 이미지와 연관된 가중치 맵.\n",
    "    - input_size: 추출할 부분 이미지의 크기.\n",
    "    - output_size: 원하는 부분 이미지의 최종 크기.\n",
    "    - is_extend: True인 경우, 패딩된 이미지 및 추출 위치 정보를 반환.\n",
    "\n",
    "    반환값:\n",
    "    - img_crop: 추출된 입력 이미지.\n",
    "    - label_crop: 추출된 레이블.\n",
    "    - wmap_crop: 추출된 가중치 맵.\n",
    "    - (옵션) img_pad: 패딩된 입력 이미지.\n",
    "    - (옵션) rand_pos_h: 추출된 부분 이미지의 세로 위치.\n",
    "    - (옵션) rand_pos_w: 추출된 부분 이미지의 가로 위치.\n",
    "    \"\"\"\n",
    "    # 입력 이미지의 경계에서 추출할 위치를 무작위로 선택\n",
    "    border_size = int((input_size - output_size) * 0.5)\n",
    "    rand_pos_h = np.random.randint(low=0, high=img.shape[0] - output_size+1, size=None)\n",
    "    rand_pos_w = np.random.randint(low=0, high=img.shape[1] - output_size+1, size=None)\n",
    "\n",
    "    # 이미지를 반사 모드로 패딩\n",
    "    img_pad = cv2.copyMakeBorder(img, border_size, border_size, border_size, border_size, cv2.BORDER_REFLECT_101)\n",
    "    \n",
    "    # 패딩된 이미지에서 추출된 위치에 해당하는 부분 이미지를 복사\n",
    "    img_crop = img_pad[rand_pos_h:rand_pos_h+input_size, rand_pos_w:rand_pos_w+input_size].copy()\n",
    "    label_crop = label[rand_pos_h:rand_pos_h+output_size, rand_pos_w:rand_pos_w+output_size].copy()\n",
    "    wmap_crop = wmap[rand_pos_h:rand_pos_h+output_size, rand_pos_w:rand_pos_w+output_size].copy()\n",
    "\n",
    "    # 패딩된 이미지 및 추출 위치 정보를 반환\n",
    "    if is_extend:\n",
    "        return img_crop, label_crop, wmap_crop, img_pad, rand_pos_h, rand_pos_w\n",
    "    \n",
    "    # 추출된 부분 이미지만 반환\n",
    "    else:\n",
    "        return img_crop, label_crop, wmap_crop\n",
    "\n",
    "def test_data_cropping(img, input_size, output_size, num_blocks=4):\n",
    "    \n",
    "    \"\"\"\n",
    "    테스트 데이터의 여러 부분 이미지를 추출하여 배치를 생성.\n",
    "\n",
    "    매개변수:\n",
    "    - img: 입력 이미지.\n",
    "    - input_size: 추출할 부분 이미지의 크기.\n",
    "    - output_size: 원하는 부분 이미지의 최종 크기.\n",
    "    - num_blocks: 생성할 부분 이미지의 개수.\n",
    "\n",
    "    반환값:\n",
    "    - x_batchs: 생성된 부분 이미지로 구성된 배치.\n",
    "    \"\"\"\n",
    "    # 부분 이미지들을 저장할 배열을 초기화\n",
    "    x_batchs = np.zeros((num_blocks, input_size, input_size), dtype=np.float32)\n",
    "\n",
    "    # 부분 이미지를 추출하기 위해 이미지를 반사 모드로 패딩\n",
    "    border_size = int((input_size - output_size) * 0.5)\n",
    "    img_pad = cv2.copyMakeBorder(img, border_size, border_size, border_size, border_size, cv2.BORDER_REFLECT_101)\n",
    "\n",
    "    # 패딩된 이미지에서 4개의 구석을 추출하여 배열에 저장\n",
    "    x_batchs[0] = img_pad[:input_size, :input_size]\n",
    "    x_batchs[1] = img_pad[:input_size, -input_size:]\n",
    "    x_batchs[2] = img_pad[-input_size:, :input_size]\n",
    "    x_batchs[3] = img_pad[-input_size:, -input_size:]\n",
    "    return x_batchs\n",
    "\n",
    "def merge_rotated_preds(preds, img, iter_time, start, stop, num, test_dir, alpha=0.6):\n",
    "    \n",
    "    \"\"\"\n",
    "    여러 회전된 예측 맵을 병합하고 시각화하여 저장.\n",
    "\n",
    "    매개변수:\n",
    "    - preds: 여러 회전된 예측 맵으로 구성된 리스트.\n",
    "    - img: 원본 이미지.\n",
    "    - iter_time: 현재 반복 횟수 또는 기타 정보.\n",
    "    - start: 회전 각도 범위의 시작.\n",
    "    - stop: 회전 각도 범위의 끝.\n",
    "    - num: 회전 각도를 나눌 개수.\n",
    "    - test_dir: 결과를 저장할 디렉토리.\n",
    "    - alpha: 시각화에서 원본 이미지의 투명도 조절.\n",
    "\n",
    "    \"\"\"\n",
    "    # 회전된 예측 맵을 저장할 배열을 초기화\n",
    "    inv_preds = np.zeros((num, *preds[0].shape), dtype=np.float32)\n",
    "\n",
    "    # 회전된 예측 맵을 역회전하여 배열에 저장\n",
    "    for i, angle in enumerate(np.linspace(start=start, stop=stop, num=num, endpoint=False)):\n",
    "        pred = preds[i].copy()\n",
    "        inv_angle = 360. - angle\n",
    "        inv_preds[i] = rotate(input=pred, angle=inv_angle, axes=(0, 1), reshape=False, order=0, mode='constant', cval=0.)\n",
    "\n",
    "    # 병합된 최종 예측 맵을 저장할 배열을 초기화\n",
    "    y_pred = np.zeros_like(inv_preds[0])\n",
    "    \n",
    "    # 각 회전된 예측 맵을 병합\n",
    "    for i in range(num):\n",
    "        y_pred += inv_preds[i]\n",
    "        \n",
    "    # 예측 맵을 클래스로 변환하고 가장 확률이 높은 클래스의 값을 255로 매핑\n",
    "    y_pred_cls = np.uint8(np.argmax(y_pred, axis=2) * 255.)\n",
    "    \n",
    "    # 가상 색상 표현을 생성\n",
    "    pseudo_label = pseudoColor(y_pred_cls)\n",
    "    beta = 1. - alpha\n",
    "\n",
    "    # 원본 이미지와 가상 색상 표현을 겹쳐서 시각화\n",
    "    img_3c = np.dstack((img, img, img))\n",
    "    overlap = cv2.addWeighted(src1=img_3c,\n",
    "                              alpha=alpha,\n",
    "                              src2=pseudo_label,\n",
    "                              beta=beta,\n",
    "                              gamma=0.0)\n",
    "\n",
    "    # test 이미지와 예측 맵에 라벨링되어 색상이 붙은 이미지를 가로로 붙여서 저장\n",
    "    canvas01 = np.hstack((img_3c, overlap))\n",
    "    cv2.imwrite(os.path.join(test_dir, 'Pred1_' + str(iter_time).zfill(2) + '.png'), canvas01)\n",
    "    \n",
    "    # test 이미지와 예측맵을 가로로 붙여서 저장\n",
    "    canvas02 = np.hstack((img, y_pred_cls))\n",
    "    cv2.imwrite(os.path.join(test_dir, 'Pred2_' + str(iter_time).zfill(2) + '.png'), canvas02)\n",
    "\n",
    "\n",
    "def merge_preds(preds, ori_size=512, output_size=388):\n",
    "    \n",
    "    \"\"\"\n",
    "    네 개의 부분 예측을 병합하여 전체 예측 맵을 생성.\n",
    "\n",
    "    매개변수:\n",
    "    - preds: 네 개의 부분 예측 맵으로 구성된 리스트.\n",
    "    - idx: 현재 반복 인덱스 또는 기타 정보.\n",
    "    - ori_size: 전체 예측 맵의 원본 크기.\n",
    "    - output_size: 각 부분 예측 맵의 크기.\n",
    "    - angle: 이미지의 회전 각도.\n",
    "    - margin: 맵 병합 시 사용할 여유 공간.\n",
    "\n",
    "    반환값:\n",
    "    - result: 병합된 전체 예측 맵.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 전체 예측 맵을 저장할 배열을 초기화\n",
    "    result = np.zeros((ori_size, ori_size, 2), dtype=np.float32)\n",
    "\n",
    "    # 각 부분 예측 맵을 해당하는 위치에 더함.\n",
    "    result[:output_size, :output_size] += preds[0]\n",
    "    result[:output_size, -output_size:] += preds[1]\n",
    "    result[-output_size:, :output_size] += preds[2]\n",
    "    result[-output_size:, -output_size:] += preds[3]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서 연산에 쓰이는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_activations(t):\n",
    "    \n",
    "    print(t.op.name, '{}', t.get_shape().as_list())\n",
    "\n",
    "def conv2d(x, output_dim, k_h=3, k_w=3, d_h=1, d_w=1, stddev=0.02, initializer='He', padding='VALID', name='conv2d',\n",
    "           is_print=True):\n",
    "    \n",
    "    # 2D 컨볼루션 연산을 수행하는 함수.\n",
    "    # 필터는 3x3 크기로 stride는 1x1 크기로 설정\n",
    "    # padding은 VALID로 설정하여 입력 이미지의 크기가 2씩 점점 줄어듬\n",
    "    \n",
    "    # 주어진 이름으로 변수 범위를 생성 변수 이름이 중복되는 것을 방지 해당 범위 내에서 정의된 모든 변수는 자동으로 생성된 범위에 속하게 됨\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "\n",
    "        # 가중치 생성 he_normal_initializer 방법을 사용하여 초기화\n",
    "        w = tf.compat.v1.get_variable('w', [k_h, k_w, x.get_shape()[-1], output_dim], initializer=tf.initializers.he_normal())\n",
    "        \n",
    "        # 가중치와 입력된 이미지에 대해 2D 컨볼루션 수행\n",
    "        conv = tf.nn.conv2d(x, w, strides=[1, d_h, d_w, 1], padding=padding)\n",
    "\n",
    "        if is_print:\n",
    "            print_activations(conv)\n",
    "        \n",
    "        return conv\n",
    "\n",
    "\n",
    "def deconv2d(x, output_dim, k_h=3, k_w=3, d_h=2, d_w=2, stddev=0.02, initializer=None, padding_='SAME',\n",
    "             output_size=None, name='deconv2d', is_print=True):\n",
    "    \n",
    "    # up convolution 연산을 수행하는 함수.\n",
    "    # 필터는 3x3 크기로 stride는 2x2 크기로 설정\n",
    "    \n",
    "    # 주어진 이름으로 변수 범위를 생성 변수 이름이 중복되는 것을 방지 해당 범위 내에서 정의된 모든 변수는 자동으로 생성된 범위에 속하게 됨\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        \n",
    "        # 입력 텐서의 shape 정보를 가져옴\n",
    "        input_shape = x.get_shape().as_list()\n",
    "\n",
    "        # 출력 크기 계산\n",
    "        h_output, w_output = None, None\n",
    "        \n",
    "        # 인풋 이미지 사이즈의 2배로 내보냄 \n",
    "        if not output_size:\n",
    "            h_output, w_output = input_shape[1] * 2, input_shape[2] * 2\n",
    "        \n",
    "        output_shape = [tf.shape(x)[0], h_output, w_output, output_dim]\n",
    "        \n",
    "        # Deconvolution을 위한 가중치 생성 he_normal_initializer 방법을 사용하여 초기화\n",
    "        w = tf.compat.v1.get_variable('w', [k_h, k_w, output_dim, input_shape[3]], initializer=tf.initializers.he_normal())\n",
    "        \n",
    "        # Deconvolution 수행 수행 결과 채널은 2배로 작아지고 이미지는 2배로 커짐\n",
    "        deconv = tf.nn.conv2d_transpose(x, w, output_shape=output_shape, strides=[1, d_h, d_w, 1], padding=padding_)\n",
    "        \n",
    "        if is_print:\n",
    "            print_activations(deconv)\n",
    "            \n",
    "        return deconv\n",
    "    \n",
    "def concat(values, axis, name='concat', is_print=True):\n",
    "    \n",
    "    # Skip connection을 위해 입력된 텐서들을 concat하는 함수.\n",
    "    \n",
    "    output = tf.concat(values=values, axis=axis, name=name)\n",
    "\n",
    "    if is_print:\n",
    "        print_activations(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "def max_pool(x, name='max_pool', ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], is_print=True):\n",
    "    \n",
    "    # Convolution 연산의 결과를 max pooling하는 함수. \n",
    "    \n",
    "    output = tf.nn.max_pool(input=x, ksize=ksize, strides=strides, padding='SAME', name=name)\n",
    "    if is_print:\n",
    "        print_activations(output)\n",
    "    return output\n",
    "\n",
    "def batch_norm(x, name='batch_norm',is_print=True):\n",
    "    \n",
    "    # convolution 연산의 결과를 batch normalization하는 함수. \n",
    "    \n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "        params_shape = [x.get_shape()[-1]]\n",
    "\n",
    "        # offset 선언 평균을 0으로 만든 다음에 데이터를 평균으로부터 얼마나 이동하는 지에 사용됨\n",
    "        beta = tf.compat.v1.get_variable('beta', params_shape, tf.float32,\n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "        # scale 선언 정규화된 값을 확장시키거나 축소시키는 역할을 함\n",
    "        gamma = tf.compat.v1.get_variable('gamma', params_shape, tf.float32,\n",
    "                                initializer=tf.constant_initializer(1.0))\n",
    "\n",
    "        # 받은 텐서를 평균과 분산 계산\n",
    "        mean, variance = tf.compat.v1.nn.moments(x=x,axes=[0, 1, 2], name='moments')\n",
    "     \n",
    "        # 받은 텐서를 평균과 분산을 이용하여 정규화 beta와 gamma를 이용하여 조절된 값으로 변환\n",
    "        y = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 1e-5)\n",
    "                \n",
    "        if is_print:\n",
    "            print_activations(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "def dropout(x, keep_prob=0.5, seed=None, name='dropout', is_print=True):\n",
    "    \n",
    "    # 과적합을 방지하기 위해 dropout을 수행하는 함수.\n",
    "    \n",
    "    output = tf.nn.dropout(x=x,\n",
    "                            rate=keep_prob,\n",
    "                            seed=tf.compat.v1.set_random_seed(seed) if seed else None,\n",
    "                            name=name)\n",
    "    if is_print:\n",
    "        print_activations(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "def relu(x, name='relu', is_print=True):\n",
    "    \n",
    "    # ReLU 활성화 함수를 수행하는 함수.\n",
    "    \n",
    "    output = tf.nn.relu(x, name=name)\n",
    "    if is_print:\n",
    "        print_activations(output)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, name='EMSegmentation'):\n",
    "        \n",
    "        self.input_size = 572\n",
    "        self.output_size = 388\n",
    "        self.input_channel = 1\n",
    "        self.input_shape = (self.input_size, self.input_size, self.input_channel)\n",
    "        self.output_shape = (self.output_size, self.output_size)\n",
    "\n",
    "        self.name = name\n",
    "        self.dataset_path = './dataset'\n",
    "\n",
    "        self.train_imgs = tiff.imread(os.path.join(self.dataset_path, 'train-volume.tif'))\n",
    "        self.train_labels = tiff.imread(os.path.join(self.dataset_path, 'train-labels.tif'))\n",
    "        self.train_wmaps = np.load(os.path.join(self.dataset_path, 'train-wmaps.npy'))\n",
    "        self.test_imgs = tiff.imread(os.path.join(self.dataset_path, 'test-volume.tif'))\n",
    "        self.mean_value = np.mean(self.train_imgs)\n",
    "\n",
    "        self.num_train = self.train_imgs.shape[0]\n",
    "        self.num_test = self.test_imgs.shape[0]\n",
    "        self.img_shape = self.train_imgs[0].shape\n",
    "\n",
    "    def random_batch(self, idx, batch_size=2):\n",
    "        \n",
    "        # 데이터 셋의 순환을 위해 인덱스를 계산\n",
    "        idx = idx % self.num_train\n",
    "        \n",
    "        # 인덱스에 해당하는 이미지와 레이블 및 가중치 맵을 가져옴\n",
    "        x_img, y_label, w_map = self.train_imgs[idx], self.train_labels[idx], self.train_wmaps[idx]\n",
    "\n",
    "        # 배치를 저장할 배열을 초기화\n",
    "        x_batchs = np.zeros((batch_size, self.input_size, self.input_size), dtype=np.float32)\n",
    "        y_batchs = np.zeros((batch_size, self.output_size, self.output_size), dtype=np.float32)\n",
    "        w_batchs = np.zeros((batch_size, self.output_size, self.output_size), dtype=np.float32)\n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "            # 이미지 랜덤하게 이동\n",
    "            x_batch, y_batch, w_batch = aug_translate(x_img, y_label, w_map)\n",
    "\n",
    "            # 이미지 랜덤하게 수평 및 수직 뒤집기\n",
    "            x_batch, y_batch, w_batch = aug_flip(x_batch, y_batch, w_batch)\n",
    "\n",
    "            # 이미지 랜덤하게 회전\n",
    "            x_batch, y_batch, w_batch = aug_rotate(x_batch, y_batch, w_batch)\n",
    "\n",
    "            # 이미지 랜덤하게 탄성 변형\n",
    "            x_batch, y_batch, w_batch = aug_elastic_deform(x_batch, y_batch, w_batch)\n",
    "\n",
    "            # unet 원본 논문을 따라\n",
    "            # 이미지 크기를 696x 696 resize하고  572 x 572 입력 이미지로 crop한다 <- input_size\n",
    "            # label 및 가중치 맵은 388 x 388 크기로 crop한다 <- output_size\n",
    "            x_batch, y_batch, w_batch = cropping(x_batch, y_batch, w_batch, self.input_size, self.output_size)\n",
    "\n",
    "            x_batchs[idx, :, :] = x_batch\n",
    "            y_batchs[idx, :, :] = y_batch\n",
    "            w_batchs[idx, :, :] = w_batch\n",
    "\n",
    "            # 생성된 배치를 반환하기 전에, 데이터를 중앙에 맞추고 0에서 1 사이의 값으로 정규화\n",
    "        return self.zero_centering(x_batchs), (y_batchs / 255).astype(np.uint8), w_batchs\n",
    "\n",
    "    def test_batch(self, idx, angle):\n",
    "        \"\"\"\n",
    "        테스트 데이터로부터 배치를 생성하는 함수.\n",
    "        \"\"\"\n",
    "        # idx에 해당하는 이미지를 가져옴\n",
    "        x_img_ori = self.test_imgs[idx]\n",
    "\n",
    "        # 지정된 각도로 이미지를 회전\n",
    "        x_img = rotate(input=x_img_ori, angle=angle, axes=(0, 1), reshape=False, order=3, mode='reflect')\n",
    "        \n",
    "        # 회전된 이미지에서 4개의 부분 이미지를 추출하여 배치를 생성\n",
    "        x_batchs = test_data_cropping(img=x_img,\n",
    "                                            input_size=self.input_size,\n",
    "                                            output_size=self.output_size,\n",
    "                                            num_blocks=4)\n",
    "        \n",
    "        return self.zero_centering(x_batchs), x_img_ori\n",
    "\n",
    "    def zero_centering(self, imgs):\n",
    "        return imgs - self.mean_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 정의\n",
    "\n",
    "기존 unet 논문에는 각각의 convolution 연산 뒤에 batch_normalization 연산을 하지 않았으나  \n",
    "구현된 여러 코드들을 찾아보니 해당 연산이 있어 추가적으로 연산을 수행함  \n",
    "dropout도 전부는 아니고 일부 추가 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, input_shape, output_shape, lr=0.001, weight_decay=1e-4, total_iters=2e4, name='U-Net'):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        # convolution 연산 채널 수\n",
    "        self.conv_dims = [64, 64, 128, 128, 256, 256, 512, 512, 1024, 1024,\n",
    "                          512, 512, 512, 256, 256, 256, 128, 128, 128, 64, 64, 64, 2]\n",
    "        self.lr = lr\n",
    "        # 가중치 값이 커져서 오버피팅이 발생하는 것을 방지하기 위해 가중치 감쇠를 적용\n",
    "        self.weight_decay = weight_decay\n",
    "        self.total_steps = total_iters\n",
    "        \n",
    "        # 특정 스텝에서 학습률을 감소시키기 위해 사용\n",
    "        self.start_decay_step = int(self.total_steps * 0.5)\n",
    "        self.decay_steps = self.total_steps - self.start_decay_step\n",
    "        self.name = name\n",
    "        self.tb_lr, self.pred = None, None\n",
    "        \n",
    "\n",
    "        with tf.compat.v1.variable_scope(self.name):\n",
    "            self._build_net()\n",
    "\n",
    "        self._tensorboard()\n",
    "\n",
    "\n",
    "    def _build_net(self):\n",
    "   \n",
    "        # placeholder: 텐서플로우에서 사용하는 입력값을 받는 매개변수 미리 공간을 만들어 놓는다.\n",
    "        # 각각의 placeholder는 입력 이미지, 레이블, 가중치 맵, dropout 확률, 검증 정확도를 저장\n",
    "        self.inp_img = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, *self.input_shape], name='input_img')\n",
    "        self.out_img = tf.compat.v1.placeholder(dtype=tf.uint8, shape=[None, *self.output_shape], name='output_img')\n",
    "        self.weight_map = tf.compat.v1.placeholder(dtype=tf.float32, shape=[None, *self.output_shape], name='weight_map')\n",
    "        self.keep_prob = tf.compat.v1.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "        self.val_acc = tf.compat.v1.placeholder(dtype=tf.float32, name='val_acc')\n",
    "        \n",
    "        # 최고 정확도를 저장하기 위한 변수\n",
    "        self.best_acc_record = tf.compat.v1.get_variable(name='best_acc_save', dtype=tf.float32, initializer=tf.constant(0.),\n",
    "                                               trainable=False)\n",
    "\n",
    "        # class를 분류하기 위해 one-hot encoding을 수행\n",
    "        # 정답에 해당하는 인덱스의 값만 1이고 나머지는 모두 0인 인코딩 방식\n",
    "        self.out_img_one_hot = tf.one_hot(indices=self.out_img, depth=2, axis=-1, dtype=tf.float32, name='one_hot')\n",
    "\n",
    "        # U-Net building\n",
    "        self.u_net()\n",
    "        #  U-Net의 출력에서 클래스를 선택하기 위해 argmax를 사용하여 클래스 예측을 얻음. 가장 큰 값을 반환 \n",
    "        self.pred_cls = tf.math.argmax(input=self.pred, axis=-1)\n",
    "\n",
    "        # 소프트맥스 함수를 적용한 뒤 크로스 엔트로피 손실을 계산.\n",
    "        self.data_loss = tf.nn.softmax_cross_entropy_with_logits(labels=self.out_img_one_hot, logits=self.pred)\n",
    "        \n",
    "        # 각 예측에 대한 평균 손실을 계산.\n",
    "        self.avg_data_loss = tf.reduce_mean(self.data_loss)\n",
    "        \n",
    "        # 가중치 맵을 곱하여 가중치가 적용된 평균 손실을 계산.\n",
    "        self.weighted_data_loss = tf.reduce_mean(self.weight_map * self.data_loss)\n",
    "\n",
    "        # l2 정규화를 적용 가중치 값이 커지는 것을 방지 오버피팅 방지\n",
    "        self.reg_term = self.weight_decay * tf.reduce_sum(\n",
    "            [tf.nn.l2_loss(weight) for weight in tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)])\n",
    "\n",
    "        # 데이터 손실과 정규화 항을 합한 총 손실\n",
    "        self.total_loss = self.weighted_data_loss + self.reg_term\n",
    "\n",
    "        # adam optimizer를 사용하여 총 손실을 최소화\n",
    "        self.train_op = self.optimizer_fn(loss=self.total_loss, name='Adam')\n",
    "        \n",
    "        # 실제 레이블 값과 예측 레이블 값이 일치하는지 확인하여 정확도 계산 백분율로 표시\n",
    "        self.accuracy = tf.reduce_mean(\n",
    "            tf.cast(tf.math.equal(x=tf.cast(tf.math.argmax(self.out_img_one_hot, axis=-1), dtype=tf.uint8),\n",
    "                                  y=tf.cast(self.pred_cls, dtype=tf.uint8)), dtype=tf.float32)) * 100.\n",
    "        \n",
    "        # 검증 정확도가 더 높은 경우 최고 정확도를 갱신\n",
    "        self.save_best_acc_op = tf.compat.v1.assign(self.best_acc_record, value=self.val_acc)\n",
    "\n",
    "    def u_net(self):\n",
    "        # Stage 1\n",
    "        print_activations(self.inp_img)\n",
    "        s1_conv1 = conv2d(x=self.inp_img, output_dim=self.conv_dims[0], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s1_conv1')\n",
    "        s1_conv1 = batch_norm(x=s1_conv1, name='s1_batch_norm1')\n",
    "        s1_conv1 = relu(s1_conv1, name='relu_s1_conv1')\n",
    "        \n",
    "        s1_conv2 = conv2d(x=s1_conv1, output_dim=self.conv_dims[1], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s1_conv2')\n",
    "        s1_conv2 = batch_norm(x=s1_conv2, name='s1_batch_norm2')\n",
    "        s1_conv2 = relu(s1_conv2, name='relu_s1_conv2')\n",
    "\n",
    "        # Stage 2\n",
    "        s2_maxpool = max_pool(x=s1_conv2, name='s2_maxpool')\n",
    "        \n",
    "        s2_conv1 = conv2d(x=s2_maxpool, output_dim=self.conv_dims[2], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s2_conv1')\n",
    "        s2_conv1 = batch_norm(x=s2_conv1, name='s2_batch_norm1')\n",
    "        s2_conv1 = relu(s2_conv1, name='relu_s2_conv1')\n",
    "        \n",
    "        s2_conv2 = conv2d(x=s2_conv1, output_dim=self.conv_dims[3], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s2_conv2')\n",
    "        s2_conv2 = batch_norm(x=s2_conv2, name='s2_batch_norm2')\n",
    "        s2_conv2 = relu(s2_conv2, name='relu_s2_conv2')\n",
    "\n",
    "        # Stage 3\n",
    "        s3_maxpool = max_pool(x=s2_conv2, name='s3_maxpool')\n",
    "        \n",
    "        s3_conv1 = conv2d(x=s3_maxpool, output_dim=self.conv_dims[4], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s3_conv1')\n",
    "        s3_conv1 = batch_norm(x=s3_conv1, name='s3_batch_norm1')\n",
    "        s3_conv1 = relu(s3_conv1, name='relu_s3_conv1')\n",
    "        \n",
    "        s3_conv2 = conv2d(x=s3_conv1, output_dim=self.conv_dims[5], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s3_conv2')\n",
    "        s3_conv2 = batch_norm(x=s3_conv2, name='s3_batch_norm2')\n",
    "        s3_conv2 = relu(s3_conv2, name='relu_s3_conv2')\n",
    "\n",
    "        # Stage 4\n",
    "        s4_maxpool = max_pool(x=s3_conv2, name='s4_maxpool')\n",
    "        \n",
    "        s4_conv1 = conv2d(x=s4_maxpool, output_dim=self.conv_dims[6], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s4_conv1')\n",
    "        s4_conv1 = batch_norm(x=s4_conv1, name='s4_batch_norm1')\n",
    "        s4_conv1 = relu(s4_conv1, name='relu_s4_conv1')\n",
    "        \n",
    "        s4_conv2 = conv2d(x=s4_conv1, output_dim=self.conv_dims[7], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s4_conv2')\n",
    "        s4_conv2 = batch_norm(x=s4_conv2, name='s4_batch_norm2')\n",
    "        s4_conv2 = relu(s4_conv2, name='relu_s4_conv2')\n",
    "        s4_conv2_drop = dropout(x=s4_conv2, keep_prob=self.keep_prob, name='s4_conv2_dropout')\n",
    "        \n",
    "        # Stage 5\n",
    "        s5_maxpool = max_pool(x=s4_conv2_drop, name='s5_maxpool')\n",
    "        \n",
    "        s5_conv1 = conv2d(x=s5_maxpool, output_dim=self.conv_dims[8], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s5_conv1')\n",
    "        s5_conv1 = batch_norm(x=s5_conv1, name='s5_batch_norm1')\n",
    "        s5_conv1 = relu(s5_conv1, name='relu_s5_conv1')\n",
    "        \n",
    "        s5_conv2 = conv2d(x=s5_conv1, output_dim=self.conv_dims[9], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s5_conv2')\n",
    "        s5_conv2 = batch_norm(x=s5_conv2, name='s5_batch_norm2')\n",
    "        s5_conv2 = relu(s5_conv2, name='relu_s5_conv2')\n",
    "        s5_conv2_drop = dropout(x=s5_conv2, keep_prob=self.keep_prob, name='s5_conv2_dropout')\n",
    "\n",
    "        # Stage 6\n",
    "        s6_deconv1 = deconv2d(x=s5_conv2_drop, output_dim=self.conv_dims[10], k_h=2, k_w=2, initializer='He'\n",
    "                                       , name='s6_deconv1')\n",
    "\n",
    "        # Cropping\n",
    "        # encoder 이미지가 더 크기 때문에 decoder 이미지에 맞춰서 crop\n",
    "        # offset_height: 이미지의 상단에서부터 잘라낼 위치\n",
    "        # offset_width: 이미지의 왼쪽에서부터 잘라낼 위치\n",
    "        # target_height: 잘라낸 이미지의 높이\n",
    "        # target_width: 잘라낸 이미지의 너비\n",
    "        # 아래의 crop 연산들도 동일한 방식으로 수행\n",
    "        h1, w1 = s4_conv2_drop.get_shape().as_list()[1:3]\n",
    "        h2, w2 = s6_deconv1.get_shape().as_list()[1:3]\n",
    "        s4_conv2_crop = tf.image.crop_to_bounding_box(image=s4_conv2_drop,\n",
    "                                                      offset_height=int(0.5 * (h1 - h2)),\n",
    "                                                      offset_width=int(0.5 * (w1 - w2)),\n",
    "                                                      target_height=h2,\n",
    "                                                      target_width=w2)\n",
    "        print_activations(s4_conv2_crop)\n",
    "\n",
    "        s6_concat = concat(values=[s4_conv2_crop, s6_deconv1], axis=3, name='s6_concat')\n",
    "        \n",
    "        s6_conv2 = conv2d(x=s6_concat, output_dim=self.conv_dims[11], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s6_conv2')\n",
    "        s6_conv2 = batch_norm(x=s6_conv2, name='s6_batch_norm2')\n",
    "        s6_conv2 = relu(s6_conv2, name='relu_s6_conv2')\n",
    "        \n",
    "        s6_conv3 = conv2d(x=s6_conv2, output_dim=self.conv_dims[12], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s6_conv3')\n",
    "        s6_conv3 = batch_norm(x=s6_conv3, name='s6_batch_norm3')\n",
    "        s6_conv3 = relu(s6_conv3, name='relu_s6_conv3')\n",
    "\n",
    "        # Stage 7\n",
    "        s7_deconv1 = deconv2d(x=s6_conv3, output_dim=self.conv_dims[13], k_h=2, k_w=2, initializer='He',\n",
    "                                       name='s7_deconv1')\n",
    "        \n",
    "        # Cropping\n",
    "        h1, w1 = s3_conv2.get_shape().as_list()[1:3]\n",
    "        h2, w2 = s7_deconv1.get_shape().as_list()[1:3]\n",
    "        s3_conv2_crop = tf.image.crop_to_bounding_box(image=s3_conv2,\n",
    "                                                      offset_height=int(0.5 * (h1 - h2)),\n",
    "                                                      offset_width=int(0.5 * (w1 - w2)),\n",
    "                                                      target_height=h2,\n",
    "                                                      target_width=w2)\n",
    "        print_activations(s3_conv2_crop)\n",
    "\n",
    "        s7_concat = concat(values=[s3_conv2_crop, s7_deconv1], axis=3, name='s7_concat')\n",
    "        \n",
    "        s7_conv2 = conv2d(x=s7_concat, output_dim=self.conv_dims[14], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s7_conv2')\n",
    "        s7_conv2 = batch_norm(x=s7_conv2, name='s7_batch_norm2')\n",
    "        s7_conv2 = relu(s7_conv2, name='relu_s7_conv2')\n",
    "        \n",
    "        s7_conv3 = conv2d(x=s7_conv2, output_dim=self.conv_dims[15], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s7_conv3')\n",
    "        s7_conv3 = batch_norm(x=s7_conv3, name='s7_batch_norm3')\n",
    "        s7_conv3 = relu(s7_conv3, name='relu_s7_conv3')\n",
    "\n",
    "        # Stage 8\n",
    "        s8_deconv1 = deconv2d(x=s7_conv3, output_dim=self.conv_dims[16], k_h=2, k_w=2, initializer='He',\n",
    "                                       name='s8_deconv1')\n",
    "\n",
    "        # Cropping\n",
    "        h1, w1 = s2_conv2.get_shape().as_list()[1:3]\n",
    "        h2, w2 = s8_deconv1.get_shape().as_list()[1:3]\n",
    "        s2_conv2_crop = tf.image.crop_to_bounding_box(image=s2_conv2,\n",
    "                                                      offset_height=int(0.5 * (h1 - h2)),\n",
    "                                                      offset_width=int(0.5 * (w1 - w2)),\n",
    "                                                      target_height=h2,\n",
    "                                                      target_width=w2)\n",
    "        print_activations(s2_conv2_crop)\n",
    "\n",
    "        s8_concat = concat(values=[s2_conv2_crop, s8_deconv1], axis=3, name='s8_concat')\n",
    "        \n",
    "        s8_conv2 = conv2d(x=s8_concat, output_dim=self.conv_dims[17], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s8_conv2')\n",
    "        s8_conv2 = batch_norm(x=s8_conv2, name='s8_batch_norm2')\n",
    "        s8_conv2 = relu(s8_conv2, name='relu_s8_conv2')\n",
    "        \n",
    "        s8_conv3 = conv2d(x=s8_conv2, output_dim=self.conv_dims[18], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s8_conv3')\n",
    "        s8_conv3 = batch_norm(x=s8_conv3, name='s8_batch_norm3')\n",
    "        s8_conv3 = relu(s8_conv3, name='relu_conv3')\n",
    "\n",
    "        # Stage 9\n",
    "        s9_deconv1 = deconv2d(x=s8_conv3, output_dim=self.conv_dims[19], k_h=2, k_w=2, initializer='He',\n",
    "                                       name='s9_deconv1')\n",
    "\n",
    "        # Cropping\n",
    "        h1, w1 = s1_conv2.get_shape().as_list()[1:3]\n",
    "        h2, w2 = s9_deconv1.get_shape().as_list()[1:3]\n",
    "        s1_conv2_crop = tf.image.crop_to_bounding_box(image=s1_conv2,\n",
    "                                                      offset_height=int(0.5 * (h1 - h2)),\n",
    "                                                      offset_width=int(0.5 * (w1 - w2)),\n",
    "                                                      target_height=h2,\n",
    "                                                      target_width=w2)\n",
    "        print_activations(s1_conv2_crop)\n",
    "\n",
    "        s9_concat = concat(values=[s1_conv2_crop, s9_deconv1], axis=3, name='s9_concat')\n",
    "        \n",
    "        s9_conv2 = conv2d(x=s9_concat, output_dim=self.conv_dims[20], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s9_conv2')\n",
    "        s9_conv2 = batch_norm(x=s9_conv2, name='s9_batch_norm2')\n",
    "        s9_conv2 = relu(s9_conv2, name='relu_s9_conv2')\n",
    "        \n",
    "        s9_conv3 = conv2d(x=s9_conv2, output_dim=self.conv_dims[21], k_h=3, k_w=3, d_h=1, d_w=1,\n",
    "                                   padding='VALID', initializer='He', name='s9_conv3')\n",
    "        s9_conv3 = batch_norm(x=s9_conv3, name='s9_batch_norm3')\n",
    "        s9_conv3 = relu(s9_conv3, name='relu_s9_conv3')\n",
    "        \n",
    "        # 출력 레이어 필터 size를 1로 하여 1x1 convolution 연산을 수행 padding을 same으로 하여 입력과 출력의 크기를 동일하게 유지\n",
    "        self.pred = conv2d(x=s9_conv3, output_dim=self.conv_dims[22], k_h=1, k_w=1, d_h=1, d_w=1,\n",
    "                                    padding='SAME', initializer='He', name='output')\n",
    "\n",
    "    \n",
    "    def optimizer_fn(self,loss, name=None):\n",
    "        with tf.compat.v1.variable_scope(name):\n",
    "            \n",
    "            # 최적화에 사용되는 전역 스텝 수를 나타내는 변수 생성. 학습 중에 업데이트됨\n",
    "            global_step = tf.Variable(0, dtype=tf.float32, trainable=False)\n",
    "            \n",
    "            # 학습률을 조정하기 위한 변수 생성\n",
    "            start_learning_rate = self.lr\n",
    "            end_learning_rate = 0.\n",
    "            # 학습률 감소를 시작하는 스텝 수\n",
    "            start_decay_step = self.start_decay_step\n",
    "            # 감소가 완료되는 스텝 수\n",
    "            decay_steps = self.decay_steps\n",
    "\n",
    "            # 현재 스텝이 학습률 감소를 시작해야 하는 스텝 이상인지 확인.\n",
    "            # 만약 그렇다면, 학습률을 감소시키는 다항식 함수를 사용하여 학습률을 감소시킴.\n",
    "            # 그렇지 않다면 초기 학습률을 사용\n",
    "            learning_rate = (tf.where(tf.greater_equal(global_step, start_decay_step),\n",
    "                                        tf.compat.v1.train.polynomial_decay(start_learning_rate,\n",
    "                                                                global_step - start_decay_step,\n",
    "                                                                decay_steps, end_learning_rate, power=1.0),\n",
    "                                        start_learning_rate))\n",
    "            \n",
    "            self.tb_lr = tf.compat.v1.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "            # adam optimizer를 사용하여 손실 함수를 최소화\n",
    "            learn_step = tf.compat.v1.train.AdamOptimizer(learning_rate, beta1=0.99).minimize(loss, global_step=global_step)\n",
    "            \n",
    "        # 최종적으로 이 함수가 호출되면 모델이 학습을 수행하고 학습 스텝을 반환\n",
    "        return learn_step\n",
    "\n",
    "    # tensorboard에 출력할 값들을 정의\n",
    "    def _tensorboard(self):\n",
    "        self.tb_total = tf.compat.v1.summary.scalar('Loss/total', self.total_loss)\n",
    "        self.tb_data = tf.compat.v1.summary.scalar('Loss/avg_data', self.avg_data_loss)\n",
    "        self.tb_weighted_data = tf.compat.v1.summary.scalar('Loss/weighted_data', self.weighted_data_loss)\n",
    "        self.tb_reg = tf.compat.v1.summary.scalar('Loss/reg_term', self.reg_term)\n",
    "        self.summary_op = tf.compat.v1.summary.merge(\n",
    "            inputs=[self.tb_lr, self.tb_total, self.tb_data, self.tb_weighted_data, self.tb_reg])\n",
    "\n",
    "        self.val_acc_op = tf.compat.v1.summary.scalar('Acc', self.val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "solver 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    def __init__(self, sess, model, mean_value):\n",
    "        self.sess = sess\n",
    "        self.model = model\n",
    "        self.mean_value = mean_value\n",
    "\n",
    "    def train(self, x, y, wmap):\n",
    "        \n",
    "        # 받아온 배치를 feed_dict에 저장\n",
    "        feed = {\n",
    "            self.model.inp_img: np.expand_dims(x, axis=3),\n",
    "            self.model.out_img: y,\n",
    "            self.model.weight_map: wmap,\n",
    "            self.model.keep_prob: 0.5\n",
    "        }\n",
    "        # 이 연산을 실행하면 학습이 되고 가중치가 업데이트\n",
    "        train_op = self.model.train_op\n",
    "        # 손실 함수의 값과 정확도를 계산\n",
    "        total_loss = self.model.total_loss\n",
    "        avg_data_loass = self.model.avg_data_loss\n",
    "        weighted_data_loss = self.model.weighted_data_loss\n",
    "        reg_term = self.model.reg_term\n",
    "        pred_cls = self.model.pred_cls\n",
    "        summary = self.model.summary_op\n",
    "\n",
    "        # 학습을 수행하고 손실 함수의 값과 정확도를 반환\n",
    "        return self.sess.run([train_op, total_loss, avg_data_loass, weighted_data_loss, reg_term, summary, pred_cls],\n",
    "                             feed_dict=feed)\n",
    "\n",
    "    def evalate(self, x, y, batch_size=4):\n",
    "        \n",
    "        print(' [*] Evaluation...')\n",
    "\n",
    "        num_test = x.shape[0]\n",
    "        avg_acc = 0.\n",
    "\n",
    "        for i_start in range(0, num_test, batch_size):\n",
    "            if i_start + batch_size < num_test:\n",
    "                i_end = i_start + batch_size\n",
    "            else:\n",
    "                i_end = num_test - 1\n",
    "\n",
    "            x_batch = x[i_start:i_end]\n",
    "            y_batch = y[i_start:i_end]\n",
    "            \n",
    "            # label image로 검증\n",
    "            feed = {\n",
    "                self.model.inp_img: np.expand_dims(x_batch, axis=3),\n",
    "                self.model.out_img: y_batch,\n",
    "                self.model.keep_prob: 1.0\n",
    "            }\n",
    "            # 입력된 데이터와 레이블로부터 정확도를 계산\n",
    "            acc_op = self.model.accuracy\n",
    "            avg_acc += self.sess.run(acc_op, feed_dict=feed)\n",
    "\n",
    "        # 배치 사이즈로 나눠서 평균 정확도 계산\n",
    "        avg_acc = np.float32(avg_acc / np.ceil(num_test / batch_size))\n",
    "        summary = self.sess.run(self.model.val_acc_op, feed_dict={self.model.val_acc: avg_acc})\n",
    "\n",
    "        return avg_acc, summary\n",
    "\n",
    "    def test(self, x):\n",
    "        # 테스트 데이터 입력받아 feed_dict에 저장\n",
    "        feed = {\n",
    "            self.model.inp_img: np.expand_dims(x, axis=3),\n",
    "            self.model.keep_prob: 1.0\n",
    "        }\n",
    "        # 예측값을 계산\n",
    "        preds = self.sess.run(self.model.pred, feed_dict=feed)\n",
    "        \n",
    "        # 4개의 구석의 부분 예측을 합쳐서 하나의 예측으로 만듦\n",
    "        pred = merge_preds(preds)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def init(self):\n",
    "        # 전역 변수 초기화\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메인 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x2b5d64ab110>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution() \n",
    "\n",
    "FLAGS = tf.compat.v1.flags.FLAGS\n",
    "tf.compat.v1.flags.DEFINE_string('dataset', 'EMSegmentation', 'dataset name, default: EMSegmentation')\n",
    "tf.compat.v1.flags.DEFINE_integer('batch_size', 4, '한번 학습에 사용되는 batch size, default: 4')\n",
    "tf.compat.v1.flags.DEFINE_float('learning_rate', 1e-5, '초기 학습률, default: 0.001')\n",
    "tf.compat.v1.flags.DEFINE_float('weight_decay', 1e-4, '오버피팅 방지를 위한 weight 감쇠, default: 0.0001')\n",
    "tf.compat.v1.flags.DEFINE_integer('iters', 10, '반복 횟수, default: 10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U-Net/input_img {} [None, 572, 572, 1]\n",
      "U-Net/s1_conv1/Conv2D {} [None, 570, 570, 64]\n",
      "U-Net/s1_batch_norm1/batchnorm/add_1 {} [None, 570, 570, 64]\n",
      "U-Net/relu_s1_conv1 {} [None, 570, 570, 64]\n",
      "U-Net/s1_conv2/Conv2D {} [None, 568, 568, 64]\n",
      "U-Net/s1_batch_norm2/batchnorm/add_1 {} [None, 568, 568, 64]\n",
      "U-Net/relu_s1_conv2 {} [None, 568, 568, 64]\n",
      "U-Net/s2_maxpool {} [None, 284, 284, 64]\n",
      "U-Net/s2_conv1/Conv2D {} [None, 282, 282, 128]\n",
      "U-Net/s2_batch_norm1/batchnorm/add_1 {} [None, 282, 282, 128]\n",
      "U-Net/relu_s2_conv1 {} [None, 282, 282, 128]\n",
      "U-Net/s2_conv2/Conv2D {} [None, 280, 280, 128]\n",
      "U-Net/s2_batch_norm2/batchnorm/add_1 {} [None, 280, 280, 128]\n",
      "U-Net/relu_s2_conv2 {} [None, 280, 280, 128]\n",
      "U-Net/s3_maxpool {} [None, 140, 140, 128]\n",
      "U-Net/s3_conv1/Conv2D {} [None, 138, 138, 256]\n",
      "U-Net/s3_batch_norm1/batchnorm/add_1 {} [None, 138, 138, 256]\n",
      "U-Net/relu_s3_conv1 {} [None, 138, 138, 256]\n",
      "U-Net/s3_conv2/Conv2D {} [None, 136, 136, 256]\n",
      "U-Net/s3_batch_norm2/batchnorm/add_1 {} [None, 136, 136, 256]\n",
      "U-Net/relu_s3_conv2 {} [None, 136, 136, 256]\n",
      "U-Net/s4_maxpool {} [None, 68, 68, 256]\n",
      "U-Net/s4_conv1/Conv2D {} [None, 66, 66, 512]\n",
      "U-Net/s4_batch_norm1/batchnorm/add_1 {} [None, 66, 66, 512]\n",
      "U-Net/relu_s4_conv1 {} [None, 66, 66, 512]\n",
      "U-Net/s4_conv2/Conv2D {} [None, 64, 64, 512]\n",
      "U-Net/s4_batch_norm2/batchnorm/add_1 {} [None, 64, 64, 512]\n",
      "U-Net/relu_s4_conv2 {} [None, 64, 64, 512]\n",
      "U-Net/s4_conv2_dropout/SelectV2 {} [None, 64, 64, 512]\n",
      "U-Net/s5_maxpool {} [None, 32, 32, 512]\n",
      "U-Net/s5_conv1/Conv2D {} [None, 30, 30, 1024]\n",
      "U-Net/s5_batch_norm1/batchnorm/add_1 {} [None, 30, 30, 1024]\n",
      "U-Net/relu_s5_conv1 {} [None, 30, 30, 1024]\n",
      "U-Net/s5_conv2/Conv2D {} [None, 28, 28, 1024]\n",
      "U-Net/s5_batch_norm2/batchnorm/add_1 {} [None, 28, 28, 1024]\n",
      "U-Net/relu_s5_conv2 {} [None, 28, 28, 1024]\n",
      "U-Net/s5_conv2_dropout/SelectV2 {} [None, 28, 28, 1024]\n",
      "U-Net/s6_deconv1/conv2d_transpose {} [None, 56, 56, 512]\n",
      "U-Net/crop_to_bounding_box/Slice {} [None, 56, 56, 512]\n",
      "U-Net/s6_concat {} [None, 56, 56, 1024]\n",
      "U-Net/s6_conv2/Conv2D {} [None, 54, 54, 512]\n",
      "U-Net/s6_batch_norm2/batchnorm/add_1 {} [None, 54, 54, 512]\n",
      "U-Net/relu_s6_conv2 {} [None, 54, 54, 512]\n",
      "U-Net/s6_conv3/Conv2D {} [None, 52, 52, 512]\n",
      "U-Net/s6_batch_norm3/batchnorm/add_1 {} [None, 52, 52, 512]\n",
      "U-Net/relu_s6_conv3 {} [None, 52, 52, 512]\n",
      "U-Net/s7_deconv1/conv2d_transpose {} [None, 104, 104, 256]\n",
      "U-Net/crop_to_bounding_box_1/Slice {} [None, 104, 104, 256]\n",
      "U-Net/s7_concat {} [None, 104, 104, 512]\n",
      "U-Net/s7_conv2/Conv2D {} [None, 102, 102, 256]\n",
      "U-Net/s7_batch_norm2/batchnorm/add_1 {} [None, 102, 102, 256]\n",
      "U-Net/relu_s7_conv2 {} [None, 102, 102, 256]\n",
      "U-Net/s7_conv3/Conv2D {} [None, 100, 100, 256]\n",
      "U-Net/s7_batch_norm3/batchnorm/add_1 {} [None, 100, 100, 256]\n",
      "U-Net/relu_s7_conv3 {} [None, 100, 100, 256]\n",
      "U-Net/s8_deconv1/conv2d_transpose {} [None, 200, 200, 128]\n",
      "U-Net/crop_to_bounding_box_2/Slice {} [None, 200, 200, 128]\n",
      "U-Net/s8_concat {} [None, 200, 200, 256]\n",
      "U-Net/s8_conv2/Conv2D {} [None, 198, 198, 128]\n",
      "U-Net/s8_batch_norm2/batchnorm/add_1 {} [None, 198, 198, 128]\n",
      "U-Net/relu_s8_conv2 {} [None, 198, 198, 128]\n",
      "U-Net/s8_conv3/Conv2D {} [None, 196, 196, 128]\n",
      "U-Net/s8_batch_norm3/batchnorm/add_1 {} [None, 196, 196, 128]\n",
      "U-Net/relu_conv3 {} [None, 196, 196, 128]\n",
      "U-Net/s9_deconv1/conv2d_transpose {} [None, 392, 392, 64]\n",
      "U-Net/crop_to_bounding_box_3/Slice {} [None, 392, 392, 64]\n",
      "U-Net/s9_concat {} [None, 392, 392, 128]\n",
      "U-Net/s9_conv2/Conv2D {} [None, 390, 390, 64]\n",
      "U-Net/s9_batch_norm2/batchnorm/add_1 {} [None, 390, 390, 64]\n",
      "U-Net/relu_s9_conv2 {} [None, 390, 390, 64]\n",
      "U-Net/s9_conv3/Conv2D {} [None, 388, 388, 64]\n",
      "U-Net/s9_batch_norm3/batchnorm/add_1 {} [None, 388, 388, 64]\n",
      "U-Net/relu_s9_conv3 {} [None, 388, 388, 64]\n",
      "U-Net/output/Conv2D {} [None, 388, 388, 2]\n",
      "0/3: \tTotal loss: 2.631, \tAvg. data loss: 0.882, \tWeighted data loss: 1.555 \tReg. term: 1.076\n",
      " [*] Evaluation...\n",
      "Evaluation! \tAcc: 57.015 \tBest Acc: 0.000\n",
      "1/3: \tTotal loss: 2.324, \tAvg. data loss: 0.790, \tWeighted data loss: 1.248 \tReg. term: 1.075\n",
      " [*] Evaluation...\n",
      "Evaluation! \tAcc: 62.151 \tBest Acc: 57.015\n",
      "2/3: \tTotal loss: 2.186, \tAvg. data loss: 0.718, \tWeighted data loss: 1.111 \tReg. term: 1.075\n",
      " [*] Evaluation...\n",
      "Evaluation! \tAcc: 64.099 \tBest Acc: 62.151\n",
      "iter: 0\n",
      "iter: 1\n",
      "iter: 2\n",
      "iter: 3\n",
      "iter: 4\n",
      "iter: 5\n",
      "iter: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\programming\\unet\\main.ipynb 셀 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mfinished!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mapp\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\tensorflow\\python\\platform\\app.py:36\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m\"\"\"Runs the program with an optional 'main' function and 'argv' list.\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m main \u001b[39m=\u001b[39m main \u001b[39mor\u001b[39;00m _sys\u001b[39m.\u001b[39mmodules[\u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmain\n\u001b[1;32m---> 36\u001b[0m _run(main\u001b[39m=\u001b[39;49mmain, argv\u001b[39m=\u001b[39;49margv, flags_parser\u001b[39m=\u001b[39;49m_parse_flags_tolerate_undef)\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\absl\\app.py:308\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, argv, flags_parser)\u001b[0m\n\u001b[0;32m    306\u001b[0m   callback()\n\u001b[0;32m    307\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 308\u001b[0m   _run_main(main, args)\n\u001b[0;32m    309\u001b[0m \u001b[39mexcept\u001b[39;00m UsageError \u001b[39mas\u001b[39;00m error:\n\u001b[0;32m    310\u001b[0m   usage(shorthelp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, detailed_error\u001b[39m=\u001b[39merror, exitcode\u001b[39m=\u001b[39merror\u001b[39m.\u001b[39mexitcode)\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\absl\\app.py:254\u001b[0m, in \u001b[0;36m_run_main\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    252\u001b[0m   sys\u001b[39m.\u001b[39mexit(profiler\u001b[39m.\u001b[39mruncall(main, argv))\n\u001b[0;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 254\u001b[0m   sys\u001b[39m.\u001b[39mexit(main(argv))\n",
      "\u001b[1;32mc:\\programming\\unet\\main.ipynb 셀 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# 학습 및 테스트 수행\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m train(data, solver,log_dir)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m test(data, solver,test_dir)\n",
      "\u001b[1;32mc:\\programming\\unet\\main.ipynb 셀 20\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, angle \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(np\u001b[39m.\u001b[39mlinspace(start\u001b[39m=\u001b[39mstart, stop\u001b[39m=\u001b[39mstop, num\u001b[39m=\u001b[39mnum, endpoint\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m     x_batchs, x_ori_img \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mtest_batch(iter_time, angle)  \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     y_preds[i] \u001b[39m=\u001b[39m solver\u001b[39m.\u001b[39;49mtest(x_batchs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# 7개의 예측값을 병합\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39m# 최종 예측은 7개의 예측값의 평균\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# 병합된 예측을 통해 이미지 세그멘테이션 수행\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m# 두 번째 이미지는 경계선을 표시한 이미지\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39m# 각각의 이미지를 test 폴더에 저장\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m merge_rotated_preds(y_preds, x_ori_img, iter_time, start, stop, num, test_dir)\n",
      "\u001b[1;32mc:\\programming\\unet\\main.ipynb 셀 20\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m feed \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39minp_img: np\u001b[39m.\u001b[39mexpand_dims(x, axis\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mkeep_prob: \u001b[39m1.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# 예측값을 계산\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msess\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpred, feed_dict\u001b[39m=\u001b[39;49mfeed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# 4개의 구석의 부분 예측을 합쳐서 하나의 예측으로 만듦\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/programming/unet/main.ipynb#X25sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m pred \u001b[39m=\u001b[39m merge_preds(preds)\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:969\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    966\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    970\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    971\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[0;32m    972\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1192\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[39m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m final_fetches \u001b[39mor\u001b[39;00m final_targets \u001b[39mor\u001b[39;00m (handle \u001b[39mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1192\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_run(handle, final_targets, final_fetches,\n\u001b[0;32m   1193\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1194\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1195\u001b[0m   results \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1372\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1372\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m   1373\u001b[0m                        run_metadata)\n\u001b[0;32m   1374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1375\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1379\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_call\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs):\n\u001b[0;32m   1378\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1379\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   1380\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1381\u001b[0m     message \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(e\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1362\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1360\u001b[0m   \u001b[39m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1362\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m   1363\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[1;32mc:\\python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1455\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1453\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_tf_sessionrun\u001b[39m(\u001b[39mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1454\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1455\u001b[0m   \u001b[39mreturn\u001b[39;00m tf_session\u001b[39m.\u001b[39;49mTF_SessionRun_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session, options, feed_dict,\n\u001b[0;32m   1456\u001b[0m                                           fetch_list, target_list,\n\u001b[0;32m   1457\u001b[0m                                           run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(_):  \n",
    "    # 현재 시간을 기준으로 로그 및 테스트 결과를 저장할 폴더 생성  \n",
    "    cur_time = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    log_dir , test_dir = make_folders(cur_time=cur_time)\n",
    "    \n",
    "    # 데이터셋 로드\n",
    "    data = Dataset(name=FLAGS.dataset)\n",
    "\n",
    "    # 세션 생성\n",
    "    sess = tf.compat.v1.Session()\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = Model(input_shape=data.input_shape,\n",
    "                  output_shape=data.output_shape,\n",
    "                  lr=FLAGS.learning_rate,\n",
    "                  weight_decay=FLAGS.weight_decay,\n",
    "                  total_iters=FLAGS.iters,\n",
    "                  name='U-Net')\n",
    "\n",
    "    # solver 생성\n",
    "    solver = Solver(sess, model, data.mean_value)\n",
    "\n",
    "    # 학습 및 테스트 수행\n",
    "    train(data, solver,log_dir)\n",
    "    \n",
    "    test(data, solver,test_dir)\n",
    "\n",
    "\n",
    "def train(data, solver, log_dir):\n",
    "    best_acc = 0.\n",
    "    num_evals = 0\n",
    "    tb_writer = tf.compat.v1.summary.FileWriter(log_dir, graph=solver.sess.graph)\n",
    "    # 전역변수 초기화\n",
    "    solver.init()\n",
    "\n",
    "    iter_time = 0\n",
    "\n",
    "    # 학습 시작\n",
    "    for iter_time in range(iter_time, FLAGS.iters):\n",
    "        # 데이터셋으로부터 배치를 생성 idx는 데이터셋의 순환을 위해 사용\n",
    "        x_batch, y_batch, w_batch = data.random_batch(batch_size=FLAGS.batch_size, idx=iter_time)\n",
    "        # 학습 수행\n",
    "        # x_batch: 입력 이미지\n",
    "        # y_batch: 레이블 이미지\n",
    "        # w_batch: 가중치 맵\n",
    "        _, total_loss, avg_data_loss, weighted_data_loss, reg_term, summary, pred_cls = \\\n",
    "            solver.train(x_batch, y_batch, w_batch)\n",
    "\n",
    "        # tensorboard에 출력\n",
    "        tb_writer.add_summary(summary, iter_time)\n",
    "        tb_writer.flush()\n",
    "\n",
    "        msg = '{}/{}: \\tTotal loss: {:.3f}, \\tAvg. data loss: {:.3f}, \\tWeighted data loss: {:.3f} \\tReg. term: {:.3f}'\n",
    "        print(msg.format(iter_time, FLAGS.iters, total_loss, avg_data_loss, weighted_data_loss, reg_term))\n",
    "\n",
    "        # 검증 정확도 계산\n",
    "        x_batch, y_batch, w_batch = data.random_batch(batch_size=FLAGS.batch_size * 20,\n",
    "                                                        idx=np.random.randint(low=0, high=FLAGS.iters))\n",
    "        acc, summary = solver.evalate(x_batch, y_batch, batch_size=FLAGS.batch_size)\n",
    "        print('Evaluation! \\tAcc: {:.3f} \\tBest Acc: {:.3f}'.format(acc, best_acc))\n",
    "\n",
    "        # tensorboard에 출력\n",
    "        tb_writer.add_summary(summary, num_evals)\n",
    "        tb_writer.flush()\n",
    "        num_evals += 1\n",
    "        \n",
    "        if acc > best_acc:\n",
    "                best_acc = acc\n",
    "\n",
    "def test(data, solver, test_dir, start=0, stop=360, num=7):\n",
    "\n",
    "    # 테스트 데이터셋의 개수만큼 반복\n",
    "    for iter_time in range(data.num_test):\n",
    "        print('iter: {}'.format(iter_time))\n",
    "        \n",
    "        # 7개의 각도로 회전된 예측값을 저장할 배열을 초기화\n",
    "        y_preds = np.zeros((num, *data.img_shape, 2), dtype=np.float32)  # [N, H, W, 2]\n",
    "        x_ori_img = None\n",
    "        \n",
    "        # 지정된 각도로 이미지를 회전\n",
    "        # 회전된 이미지에서 4개의 부분 이미지를 추출하여 배치를 생성\n",
    "        # 생성된 배치를 모델에 입력하여 예측값을 얻음\n",
    "        # 부분 예측을 하나의 예측으로 병합\n",
    "        # 병합된 예측을 저장\n",
    "        # 위의 과정을 지정된 각도만큼 반복\n",
    "        for i, angle in enumerate(np.linspace(start=start, stop=stop, num=num, endpoint=False)):\n",
    "            x_batchs, x_ori_img = data.test_batch(iter_time, angle)  \n",
    "            y_preds[i] = solver.test(x_batchs)\n",
    "\n",
    "        # 7개의 예측값을 병합\n",
    "        # 최종 예측은 7개의 예측값의 평균\n",
    "        # 병합된 예측을 통해 이미지 세그멘테이션 수행\n",
    "        # 총 2개의 이미지를 생성\n",
    "        # 첫 번째 이미지는 경계선을 기준으로 라벨링 된 이미지에 가상의 색상을 입힌 이미지\n",
    "        # 두 번째 이미지는 경계선을 표시한 이미지\n",
    "        # 각각의 이미지를 test 폴더에 저장\n",
    "        merge_rotated_preds(y_preds, x_ori_img, iter_time, start, stop, num, test_dir)\n",
    "    print('finished!')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.compat.v1.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습은 19분 정도 소요  \n",
    "test image로 예측해 이미지 세그멘테이션하는 것은 14분 정도 소요  \n",
    "\n",
    "총 33분 ~ 40분 소요\n",
    "\n",
    "텐서보드로 정확도 로스 학습률 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 8412), started 0:11:09 ago. (Use '!kill 8412' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2957097925ec8753\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2957097925ec8753\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --port=6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
